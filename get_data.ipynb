{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ""
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from collections import namedtuple\n",
    "# import validators\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from halo import Halo\n",
    "def get_soup(url):\n",
    "    '''\n",
    "    Returns a BeautifulSoup version of a web page.\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "    return BeautifulSoup(content, features='html.parser')\n",
    "\n",
    "def get_links(table):\n",
    "    '''\n",
    "    Retrieves links from an embedded tabular structure.\n",
    "    '''\n",
    "    links = set()\n",
    "    for child in table.children:\n",
    "        for link in child.find_all('a', href=True):\n",
    "            links.add(link['href'])\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "⠋ Fetching article list . . .⠙ Fetching article list . . .⠹ Fetching article list . . .⠸ Fetching article list . . .⠼ Fetching article list . . .⠴ Fetching article list . . .⠦ Fetching article list . . .⠧ Fetching article list . . .⠇ Fetching article list . . .✔ Fetching list ===> Done\n⠋ Processing articles . . .⠙ Processing articles . . ."
    }
   ],
   "source": [
    "url = 'http://christmas-specials.wikia.com/'\n",
    "    #data='xmas'\n",
    "    # if not validators.url(url):\n",
    "    #     print('Invalid URL')\n",
    "    # path = pathlib.Path(data)\n",
    "\n",
    "spinner = Halo(text='Fetching article list . . .')\n",
    "spinner.start()\n",
    "    \n",
    "soup = get_soup(url + '/wiki/Special:AllPages')\n",
    "spinner.succeed(text='Fetching list ===> Done')\n",
    "\n",
    "'''Process the articles'''\n",
    "\n",
    "articles = set()\n",
    "\n",
    "spinner = Halo(text='Processing articles . . .')\n",
    "spinner.start()\n",
    "\n",
    "table_chunks = soup.find('table', \n",
    "        {'class': ['allpageslist', 'mw-allpages-table-chunk']})\n",
    "    \n",
    "chunks = set(get_links(table_chunks))\n",
    "\n",
    "chunks_done = 0\n",
    "chunks_left = len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ""
    }
   ],
   "source": [
    "# proof of concept\n",
    "\n",
    "article = '/wiki/Mr._Monk_and_the_Man_Who_Shot_Santa_Claus'\n",
    "\n",
    "def get_article_data(article, url='http://christmas-specials.wikia.com/'):\n",
    "    Article = namedtuple('Article', 'title contents categories related')\n",
    "    article_soup = get_soup(url + article)\n",
    "    article_title = article_soup.find('h1').text\n",
    "    # article_contents = article_soup.find_all('div', {'class': ['mw-content-ltr', 'mw-content-text', 'mw-collapsible', 'mw-made-collapsible']})\n",
    "    related = []\n",
    "    for tag in soup.select('h2 ~ ul > li'):\n",
    "        related.append((tag.text, tag.a['href']))\n",
    "\n",
    "    article_contents = ' '.join([p.text for p in article_soup.find_all('p')])\n",
    "    article_categories = [li['data-name'] for li in article_soup.find_all('li', {'class': 'category normal', 'data-type': 'normal'})]\n",
    "    article = Article(title=article_title, contents=article_contents, categories=article_categories, related=related)\n",
    "    return article\n",
    "\n",
    "monk = get_article_data(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'dict'>\n"
    }
   ],
   "source": [
    "#print(monk.title)\n",
    "#print(monk.categories)\n",
    "#print(monk.related)\n",
    "#print(monk.contents)\n",
    "print(type(monk._asdict()))\n",
    "with open('monk.json', 'w') as outfile:\n",
    "    json.dump(monk._asdict(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "✔ Processing chunks ===> Done\n"
    },
    {
     "data": {
      "text/plain": "<halo.halo.Halo at 0x104250c40>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spinner.text = f'Processing chunks: {chunks_done}/{chunks_left}'\n",
    "\n",
    "article_data = []\n",
    "'''Process the chunks'''\n",
    "while len(chunks) > 0:\n",
    "    current_link = chunks.pop()\n",
    "    soup = get_soup(url + current_link)\n",
    "    current_table = soup.find('table', \n",
    "    {'class': ['allpageslist', 'mw-allpages-table-chunk']})\n",
    "    if 'allpageslist' in current_table.get('class'):\n",
    "        child_chunks = get_links(current_table)\n",
    "        chunks = chunks.union(child_chunks)\n",
    "        chunks_left += len(child_chunks)\n",
    "    if 'mw-allpages-table-chunk' in current_table.get('class'):\n",
    "        for child in current_table.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                continue\n",
    "            else:\n",
    "                for link in child.find_all('a', href=True):\n",
    "                    article_link = link['href']\n",
    "                    # article = article_link.split('/')[-1]\n",
    "                    # articles.add((article, article.replace('_', ' ')))\n",
    "                    try:\n",
    "                        data = get_article_data(article_link)\n",
    "                        article_data.append(data)\n",
    "                    except Exception as e:\n",
    "                        print(e, article_link)\n",
    "      \n",
    "    chunks_done += 1\n",
    "    spinner.text = f'Processing chunks: {chunks_done}/{chunks_left}'\n",
    "\n",
    "spinner.succeed(text='Processing chunks ===> Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "⠋ Printing 8684 articles⠙ Printing 8684 articles✔ Printing 8684 articles. ===> Done\n"
    },
    {
     "data": {
      "text/plain": "<halo.halo.Halo at 0x10a5c2940>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spinner = Halo(text=f'Printing {len(articles)} articles')\n",
    "spinner.start()\n",
    "\n",
    "# for article in sorted(articles):\n",
    "#    with open(f'{article}.txt', 'w') as outfile:\n",
    "#        outfile.write(article)\n",
    "#        outfile.write('\\n')\n",
    "\n",
    "spinner.succeed(text=f'Printing {len(articles)} articles. ===> Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}